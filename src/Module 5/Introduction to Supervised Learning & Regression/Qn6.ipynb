{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Understanding Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Calculate MAE and MSE on test predictions and compare errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.15\n",
      "Mean Squared Error (MSE): 0.03\n",
      "MAE is greater than or equal to MSE, which is uncommon unless errors are very small or uniform.\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Calculate MAE and MSE on test predictions and compare errors\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Example actual and predicted values (test set)\n",
    "y_test = np.array([3.0, 5.0, 2.5, 7.0])\n",
    "y_pred = np.array([2.8, 4.9, 2.7, 7.1])\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "\n",
    "# Comparison\n",
    "if mse > mae:\n",
    "    print(\"MSE is greater than MAE, indicating larger errors are penalized more in MSE.\")\n",
    "else:\n",
    "    print(\"MAE is greater than or equal to MSE, which is uncommon unless errors are very small or uniform.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Evaluate R2 Score on varying datasets and discuss significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect prediction R2 Score: 1.00 (Perfect fit)\n",
      "Good prediction R2 Score: 0.99 (High, but not perfect)\n",
      "Poor prediction R2 Score: -1.00 (Low or negative)\n",
      "\n",
      "R2 Score (coefficient of determination) measures how well predictions approximate actual values.\n",
      "R2 = 1 means perfect prediction, R2 = 0 means model predicts no better than the mean, and R2 < 0 means the model is worse than predicting the mean.\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Evaluate R2 Score on varying datasets and discuss significance\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Perfect prediction\n",
    "y_true1 = np.array([10, 20, 30, 40])\n",
    "y_pred1 = np.array([10, 20, 30, 40])\n",
    "r2_1 = r2_score(y_true1, y_pred1)\n",
    "print(f\"Perfect prediction R2 Score: {r2_1:.2f} (Perfect fit)\")\n",
    "\n",
    "# Good prediction\n",
    "y_true2 = np.array([10, 20, 30, 40])\n",
    "y_pred2 = np.array([12, 19, 29, 41])\n",
    "r2_2 = r2_score(y_true2, y_pred2)\n",
    "print(f\"Good prediction R2 Score: {r2_2:.2f} (High, but not perfect)\")\n",
    "\n",
    "# Poor prediction\n",
    "y_true3 = np.array([10, 20, 30, 40])\n",
    "y_pred3 = np.array([30, 10, 40, 20])\n",
    "r2_3 = r2_score(y_true3, y_pred3)\n",
    "print(f\"Poor prediction R2 Score: {r2_3:.2f} (Low or negative)\")\n",
    "\n",
    "# Discussion:\n",
    "print(\"\\nR2 Score (coefficient of determination) measures how well predictions approximate actual values.\")\n",
    "print(\"R2 = 1 means perfect prediction, R2 = 0 means model predicts no better than the mean, and R2 < 0 means the model is worse than predicting the mean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Use a sample dataset, compute all three metrics, and deduce model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 9.00\n",
      "Mean Squared Error (MSE): 85.00\n",
      "R2 Score: 0.98\n",
      "Model performance is strong: predictions closely match actual values.\n",
      "MAE and MSE indicate the average and squared average error, respectively. Lower values mean better performance.\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Use a sample dataset, compute all three metrics, and deduce model performance\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset: actual and predicted values\n",
    "y_true = np.array([100, 150, 200, 250, 300])\n",
    "y_pred = np.array([110, 140, 195, 260, 290])\n",
    "\n",
    "# Compute metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R2 Score: {r2:.2f}\")\n",
    "\n",
    "# Deduction\n",
    "if r2 > 0.8:\n",
    "    print(\"Model performance is strong: predictions closely match actual values.\")\n",
    "elif r2 > 0.5:\n",
    "    print(\"Model performance is moderate: predictions are reasonably close to actual values.\")\n",
    "else:\n",
    "    print(\"Model performance is weak: predictions do not match actual values well.\")\n",
    "\n",
    "print(\"MAE and MSE indicate the average and squared average error, respectively. Lower values mean better performance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
